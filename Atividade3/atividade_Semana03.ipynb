{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entregável 3 de Visão Computacional e Robótica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrega até 12/03 ao fim do atendimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode ser feito **em duplas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta semana vamos trabalhar com um assunto extremamente atual: reconhecimento de objetos e rastreamento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referências:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://www.pyimagesearch.com/2018/07/30/opencv-object-tracking/](https://www.pyimagesearch.com/2018/07/30/opencv-object-tracking/)\n",
    "\n",
    "[https://github.com/iArunava/YOLOv3-Object-Detection-with-OpenCV/](https://github.com/iArunava/YOLOv3-Object-Detection-with-OpenCV/)\n",
    "\n",
    "[https://www.pyimagesearch.com/2017/09/11/object-detection-with-deep-learning-and-opencv/](https://www.pyimagesearch.com/2017/09/11/object-detection-with-deep-learning-and-opencv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouça a explicacão do professor sobre rastreamento e deteção"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alunos: Lucas Muchaluat e Andre Weber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Executar os três exemplos\n",
    "\n",
    "Há três exemplos: `mobilenet_detection`, `yolov3_detection` e `tracking`.\n",
    "\n",
    "Os dois primeiros são reconhecedores de objetos, e o último é de rastreamento\n",
    "\n",
    "\n",
    "Um dos arquivos abaixo precisa ser baixado e salvo nas pasta  `yolov3_detection/yolov3-coco` .\n",
    "\n",
    "[https://www.dropbox.com/s/013ogt2bhwfzxwb/yolov3.weights?dl=0](https://www.dropbox.com/s/013ogt2bhwfzxwb/yolov3.weights?dl=0) ou [https://pjreddie.com/media/files/yolov3.weights](https://pjreddie.com/media/files/yolov3.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identificar objeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolha um dos projetos: `yolov3_detection` ou `mobilenet_detection` para basear seu código. \n",
    "\n",
    "Neste projeto, escolha uma categoria de objetos que o reconhecedor reconhece. Diga aqui qual foi sua escolha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolha: CAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rastrear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemente a seguinte funcionalidade: sempre que o objeto identificado em (2) estiver presente por mais que 5 frames seguidos, pare de rodar a identificação e comece a rastreá-lo.\n",
    "\n",
    "Faça um vídeo com a  demonstração funcionalidade e mostre o link ao professor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link: https://youtu.be/UlR5qervZN4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificando objeto (carro) e trackeando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para RODAR\n",
    "# python object_detection_webcam.py --prototxt MobileNetSSD_deploy.prototxt.txt --model MobileNetSSD_deploy.caffemodel\n",
    "# Credits: https://www.pyimagesearch.com/2017/09/11/object-detection-with-deep-learning-and-opencv/\n",
    "\n",
    "print(\"Para executar:\\npython object_detection_webcam.py --prototxt MobileNetSSD_deploy.prototxt.txt --model MobileNetSSD_deploy.caffemodel\")\n",
    "\n",
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import imutils\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-p\", \"--prototxt\", required=True,\n",
    "\thelp=\"path to Caffe 'deploy' prototxt file\")\n",
    "ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "\thelp=\"path to Caffe pre-trained model\")\n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.2,\n",
    "\thelp=\"minimum probability to filter weak detections\")\n",
    "ap.add_argument(\"-v\", \"--video\", type=str,\n",
    "    help=\"path to input video file\")\n",
    "ap.add_argument(\"-t\", \"--tracker\", type=str, default=\"kcf\",\n",
    "    help=\"OpenCV object tracker type\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# initialize the list of class labels MobileNet SSD was trained to\n",
    "# detect, then generate a set of bounding box colors for each class\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "\t\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "\t\"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "\t\"sofa\", \"train\", \"tvmonitor\"]\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "\n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    "\n",
    "# extract the OpenCV version info\n",
    "(major, minor) = cv2.__version__.split(\".\")[:2]\n",
    "\n",
    "# if we are using OpenCV 3.2 OR BEFORE, we can use a special factory\n",
    "# function to create our object tracker\n",
    "if int(major) == 3 and int(minor) < 3:\n",
    "    tracker = cv2.Tracker_create(args[\"tracker\"].upper())\n",
    "\n",
    "# otherwise, for OpenCV 3.3 OR NEWER, we need to explicity call the\n",
    "# approrpiate object tracker constructor:\n",
    "else:\n",
    "    # initialize a dictionary that maps strings to their corresponding\n",
    "    # OpenCV object tracker implementations\n",
    "    OPENCV_OBJECT_TRACKERS = {\n",
    "        \"csrt\": cv2.TrackerCSRT_create,\n",
    "        \"kcf\": cv2.TrackerKCF_create,\n",
    "        \"boosting\": cv2.TrackerBoosting_create,\n",
    "        \"mil\": cv2.TrackerMIL_create,\n",
    "        \"tld\": cv2.TrackerTLD_create,\n",
    "        \"medianflow\": cv2.TrackerMedianFlow_create,\n",
    "        \"mosse\": cv2.TrackerMOSSE_create\n",
    "    }\n",
    "\n",
    "    # grab the appropriate object tracker using our dictionary of\n",
    "    # OpenCV object tracker objects\n",
    "    tracker = OPENCV_OBJECT_TRACKERS[args[\"tracker\"]]()\n",
    "\n",
    "\n",
    "\n",
    "# initialize the bounding box coordinates of the object we are going\n",
    "# to track\n",
    "initBB = None\n",
    "\n",
    "# initialize the FPS throughput estimator\n",
    "fps = None\n",
    "\n",
    "\n",
    "# load the input image and construct an input blob for the image\n",
    "# by resizing to a fixed 300x300 pixels and then normalizing it\n",
    "# (note: normalization is done via the authors of the MobileNet SSD\n",
    "# implementation)\n",
    "\n",
    "\n",
    "def detect(frame):\n",
    "    image = frame.copy()\n",
    "    (h, w) = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 0.007843, (300, 300), 127.5)\n",
    "\n",
    "    # pass the blob through the network and obtain the detections and\n",
    "    # predictions\n",
    "    print(\"[INFO] computing object detections...\")\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in np.arange(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with the\n",
    "        # prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections by ensuring the `confidence` is\n",
    "        # greater than the minimum confidence\n",
    "\n",
    "\n",
    "        if confidence > args[\"confidence\"]:\n",
    "            # extract the index of the class label from the `detections`,\n",
    "            # then compute the (x, y)-coordinates of the bounding box for\n",
    "            # the object\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # display the prediction\n",
    "            label = \"{}: {:.2f}%\".format(CLASSES[idx], confidence * 100)\n",
    "            print(\"[INFO] {}\".format(label))\n",
    "            cv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "                COLORS[idx], 2)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(image, label, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
    "\n",
    "            results.append((CLASSES[idx], confidence*100, (startX, startY),(endX, endY) ))\n",
    "\n",
    "    # show the output image\n",
    "    return image, results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "#cap = cv2.VideoCapture('hall_box_battery_1024.mp4')\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Known classes\")\n",
    "print(CLASSES)\n",
    "\n",
    "count_frame = 0\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # resize the frame (so we can process it faster) and grab the frame dimensions\n",
    "    frame = imutils.resize(frame, width=500)\n",
    "    (H, W) = frame.shape[:2]\n",
    "\n",
    "    if count_frame < 5:\n",
    "        result_frame, result_tuples = detect(frame)\n",
    "        if len(result_tuples)>0:\n",
    "            if result_tuples[0][0] == \"car\":\n",
    "                count_frame += 1\n",
    "            else:\n",
    "                count_frame = 0\n",
    "    else:\n",
    "        x1 =  result_tuples[0][2][0]\n",
    "        y1 = result_tuples[0][2][1]\n",
    "        x2 = result_tuples[0][3][0]\n",
    "        y2 = result_tuples[0][3][1]\n",
    "        difx = x2 - x1\n",
    "        dify = y2 - y1\n",
    "\n",
    "        initBB = (x1, y1, abs(difx), abs(dify))\n",
    "\n",
    "        # start OpenCV object tracker using the supplied bounding box\n",
    "        # coordinates, then start the FPS throughput estimator as well\n",
    "        tracker.init(frame, initBB)\n",
    "        fps = FPS().start()\n",
    "\n",
    "        # check to see if we are currently tracking an object\n",
    "        if initBB is not None:\n",
    "            # grab the new bounding box coordinates of the object\n",
    "            (success, box) = tracker.update(frame)\n",
    "\n",
    "            # check to see if the tracking was a success\n",
    "            if success:\n",
    "                (x, y, w, h) = [int(v) for v in box]\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h),\n",
    "                    (0, 255, 0), 2)\n",
    "            # update the FPS counter\n",
    "            fps.update()\n",
    "            fps.stop()\n",
    "\n",
    "            # initialize the set of information we'll be displaying on\n",
    "            # the frame\n",
    "            info = [\n",
    "                (\"Tracker\", args[\"tracker\"]),\n",
    "                (\"Success\", \"Yes\" if success else \"No\"),\n",
    "                (\"FPS\", \"{:.2f}\".format(fps.fps())),\n",
    "            ]\n",
    "\n",
    "            # loop over the info tuples and draw them on our frame\n",
    "            for (i, (k, v)) in enumerate(info):\n",
    "                text = \"{}: {}\".format(k, v)\n",
    "                cv2.putText(frame, text, (10, H - ((i * 20) + 20)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',frame)\n",
    "\n",
    "    # Prints the structures results:\n",
    "    # Format:w\n",
    "    # (\"CLASS\", confidence, (x1, y1, x2, y3))\n",
    "    for t in result_tuples:\n",
    "        print(t)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Rode o simulador do Turtlebot (use o Waffle).  Veja o guia em [../guides/simulador_ros.md](../guides/simulador_ros.md)\n",
    " \n",
    " Documente aqui as linhas necessárias para teleop e para abrir o Rviz\n",
    " \n",
    " Faça um screenshot do seu simulação em execução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Robô quadrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça [este tutorial](../guides/projeto_rospython.md) de como criar um projeto Python que comanda o robô simulado.\n",
    "\n",
    "Usando o simulador, crie um código que faça o robô fazer uma trajetória que aproxima um quadrado.\n",
    "\n",
    "Baseie-se no código `roda.py`, construído durante o tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import rospy\n",
    "from geometry_msgs.msg import Twist, Vector3\n",
    "from math import pi\n",
    "\n",
    "\n",
    "v = 0.3  # Velocidade linear\n",
    "w = pi/3  # Velocidade angular\n",
    "stop = 0\n",
    "if __name__ == \"__main__\":\n",
    "    rospy.init_node(\"roda_exemplo\")\n",
    "    pub = rospy.Publisher(\"cmd_vel\", Twist, queue_size=3)\n",
    "\n",
    "    try:\n",
    "        while not rospy.is_shutdown():\n",
    "\n",
    "            vel_forw = Twist(Vector3(v,0,0), Vector3(0,0,0))\n",
    "            vel_turn = Twist(Vector3(0,0,0), Vector3(0,0,w))\n",
    "            vel_stop = Twist(Vector3(0,0,0), Vector3(0,0,0))\n",
    "\n",
    "\n",
    "\n",
    "            if stop == 0:\n",
    "                pub.publish(vel_stop)\n",
    "                rospy.sleep(2)\n",
    "\n",
    "            while stop <= 4:\n",
    "                pub.publish(vel_stop)\n",
    "                rospy.sleep(2)\n",
    "\n",
    "                pub.publish(vel_forw)\n",
    "                rospy.sleep(1.8)\n",
    "\n",
    "\n",
    "                pub.publish(vel_stop)\n",
    "                rospy.sleep(2)\n",
    "\n",
    "\n",
    "                pub.publish(vel_turn)\n",
    "                rospy.sleep(1.5)\n",
    "\n",
    "                pub.publish(vel_stop)\n",
    "                rospy.sleep(2)\n",
    "\n",
    "                stop += 1\n",
    "\n",
    "    except rospy.ROSInterruptException:\n",
    "        print(\"Ocorreu uma exceção com o rospy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Robô indeciso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando o simulador e o LIDAR simulado, faça um robô avançar quando o obstáculo bem à sua frente estiver a menos de 1.0m e recuar quando estiver a mais de 1.02 m.\n",
    "\n",
    "Baseie-se no código `le_scan.py` e `roda.py`, desenvolvidos [durante o tutorial](../guides/projeto_rospython.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import rospy\n",
    "import numpy as np\n",
    "\n",
    "from geometry_msgs.msg import Twist, Vector3\n",
    "from sensor_msgs.msg import LaserScan\n",
    "\n",
    "\n",
    "def scaneou(dado):\n",
    "    print(\"Faixa valida: \", dado.range_min , \" - \", dado.range_max )\n",
    "    print(\"Leituras:\")\n",
    "    print(np.array(dado.ranges).round(decimals=2))\n",
    "    dados = dado\n",
    "    global dados\n",
    "    #print(\"Intensities\")\n",
    "    #print(np.array(dado.intensities).round(decimals=2))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    rospy.init_node(\"le_scan\")\n",
    "\n",
    "    velocidade_saida = rospy.Publisher(\"/cmd_vel\", Twist, queue_size = 3 )\n",
    "    recebe_scan = rospy.Subscriber(\"/scan\", LaserScan, scaneou)\n",
    "\n",
    "\n",
    "\n",
    "    while not rospy.is_shutdown():\n",
    "        print(\"Oeee\")\n",
    "        vel_forw = Twist(Vector3(0.1,0,0), Vector3(0,0,0))\n",
    "        vel_back = Twist(Vector3(-0.1,0,0), Vector3(0,0,0))\n",
    "        # velocidade = Twist(Vector3(0, 0, 0), Vector3(0, 0, 1))\n",
    "        velocidade_saida.publish(vel_forw)\n",
    "        rospy.sleep(2)\n",
    "\n",
    "        if dados.ranges[0] < 1:\n",
    "            velocidade_saida.publish(vel_back)\n",
    "            rospy.sleep(2)\n",
    "        if dados.ranges[0] > 1.02:\n",
    "            velocidade_saida.publish(vel_forw)\n",
    "            rospy.sleep(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
